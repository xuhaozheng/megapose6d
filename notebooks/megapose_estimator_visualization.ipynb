{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3874bde-0969-418e-9e3d-4cf9ab930413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# sys.path.append('./src')\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39;49m), \u001b[39m'\u001b[39m\u001b[39m./src/megapose\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmegapose\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmegapose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets_cfg\u001b[39;00m \u001b[39mimport\u001b[39;00m make_scene_dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import transforms3d\n",
    "\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import sys\n",
    "# sys.path.append('./src')\n",
    "sys.path.append('/media/deeplearner/b61629f8-c4e1-4166-862c-8d088f1e3c43/haozheng/megapose6d/src/megapose')\n",
    "\n",
    "import megapose\n",
    "\n",
    "from megapose.datasets.datasets_cfg import make_scene_dataset\n",
    "from megapose.config import LOCAL_DATA_DIR, NB_DATA_DIR\n",
    "from megapose.training.utils import RGB_DIMS\n",
    "from megapose.inference.utils import make_cameras\n",
    "import pickle as pkl\n",
    "from bokeh.io import show, output_notebook; output_notebook()\n",
    "from megapose.visualization.bokeh_plotter import BokehPlotter\n",
    "from bokeh.plotting import gridplot\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import scipy\n",
    "\n",
    "\n",
    "from megapose.training.utils import cast_images, cast_to_numpy, CudaTimer\n",
    "from megapose.lib3d.camera_geometry import get_K_crop_resize\n",
    "from megapose.datasets.scene_dataset import SceneObservation\n",
    "\n",
    "from megapose.inference.pose_estimator import PoseEstimator, ObservationTensor\n",
    "from megapose.inference.icp_refiner import ICPRefiner\n",
    "from megapose.visualization.utils import adjust_brightness, tensor_image_to_uint8, \\\n",
    "get_ds_info, make_contour_overlay\n",
    "from megapose.utils import transform_utils\n",
    "from megapose.lib3d.cosypose_ops import (\n",
    "    TCO_init_from_boxes,\n",
    "    TCO_init_from_boxes_zup_autodepth,\n",
    "    TCO_init_from_boxes_autodepth_with_R\n",
    ")\n",
    "from megapose.panda3d_renderer.types import Panda3dLightData\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "BRIGHTNESS_FACTOR=1.5\n",
    "\n",
    "# zmq_url = \"tcp://127.0.0.1:6000\"\n",
    "# zmq_url = \"tcp://127.0.0.1:6001\"\n",
    "# zmq_url = \"tcp://127.0.0.1:6004\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene_data(scene_ds, scene_id, view_id):\n",
    "    df = scene_ds.frame_index\n",
    "    x = df[(df.scene_id == scene_id) & (df.view_id==view_id)]\n",
    "    ds_idx = x.iloc[0].name\n",
    "    scene_data = scene_ds[ds_idx]\n",
    "    return scene_data\n",
    "\n",
    "\n",
    "def orthogonalize_rotation(T):\n",
    "    rot = scipy.spatial.transform.Rotation.from_matrix(T[:3,:3])\n",
    "    T[:3,:3] = rot.as_matrix()\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_label = None\n",
    "\n",
    "ds_name = 'ycbv'\n",
    "scene_ds_name = f\"{ds_name}.test\"\n",
    "n_refiner_iterations = 5\n",
    "\n",
    "\n",
    "scene_id, im_idx, object_label = 54, 1, 'obj_000015' # drill\n",
    "# scene_id, im_idx, object_label = 54, 1, 'obj_000003' # sugargox\n",
    "\n",
    "\n",
    "view_id = im_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce8f1a",
   "metadata": {},
   "source": [
    "## Load data and visualize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images/data\n",
    "scene_ds_kwargs = {'load_depth': True}\n",
    "scene_ds = make_scene_dataset(scene_ds_name, **scene_ds_kwargs)\n",
    "scene_data = get_scene_data(scene_ds, scene_id, view_id)\n",
    "    \n",
    "if scene_data.depth is not None:\n",
    "    depth = torch.as_tensor(scene_data.depth).unsqueeze(-1)\n",
    "    rgb = torch.as_tensor(scene_data.rgb)\n",
    "    image = torch.cat([rgb, depth], dim=-1).numpy()\n",
    "else:\n",
    "    image = scene_data.rgb.numpy()\n",
    "\n",
    "images = [image]\n",
    "cameras = make_cameras([scene_data.camera_data])\n",
    "\n",
    "plotter = BokehPlotter()\n",
    "image_f = plotter.plot_image(images[0][...,RGB_DIMS].astype(np.uint8))\n",
    "show(image_f)\n",
    "\n",
    "\n",
    "if object_label is None:\n",
    "    object_labels = None\n",
    "else:\n",
    "    object_labels = [object_label]\n",
    "data = SceneObservation.collate_fn([scene_data], object_labels=object_labels)\n",
    "observation_tensor = ObservationTensor.from_numpy(scene_data.rgb, depth=scene_data.depth, K=scene_data.camera_data.K)\n",
    "observation_tensor = observation_tensor.cuda()\n",
    "\n",
    "\n",
    "# Filter gt_detections to only keep the object we are interested in\n",
    "gt_detections = data['gt_detections']\n",
    "# Filter and only run the estimator for that object\n",
    "df = gt_detections.infos\n",
    "df = df[df.label == object_label]\n",
    "detection_idx = df.iloc[0].name\n",
    "gt_detections = gt_detections[[detection_idx]]\n",
    "gt_detections = gt_detections.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56307a",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "Select whether to load a depth refiner or not and what type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc6509",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import megapose.inference.utils\n",
    "cfg = OmegaConf.create()\n",
    "cfg.model_name = 'sn-gso-4views-normals'\n",
    "cfg.ds_name = ds_name\n",
    "cfg.use_icp = False\n",
    "depth_multiplier = None\n",
    "per_iter_depth_multiplier = None\n",
    "model_data = megapose.inference.utils.load_named_model(cfg)\n",
    "result_name = cfg.model_name\n",
    "\n",
    "\n",
    "\n",
    "refiner_model = model_data['refiner_model']\n",
    "coarse_model = model_data['coarse_model']\n",
    "obj_ds_name = model_data['obj_ds_name']\n",
    "detector_model = model_data['detector_model']\n",
    "renderer = refiner_model.renderer\n",
    "\n",
    "mesh_db = refiner_model.mesh_db\n",
    "\n",
    "\n",
    "depth_refiner = None\n",
    "depth_refiner_type = None\n",
    "# depth_refiner_type = \"icp\"\n",
    "# depth_refiner_type = \"teaser++\"\n",
    "\n",
    "if depth_refiner_type == \"icp\":\n",
    "    depth_refiner = ICPRefiner(mesh_db, renderer)\n",
    "elif depth_refiner_type == \"teaserpp\":\n",
    "    from megapose.inference.teaserpp_refiner import TeaserppRefiner\n",
    "    depth_refiner = TeaserppRefiner(mesh_db, renderer)\n",
    "\n",
    "pose_estimator = PoseEstimator(refiner_model=refiner_model,\n",
    "                              coarse_model=coarse_model,\n",
    "                               detector_model=detector_model,\n",
    "                               depth_refiner=depth_refiner,\n",
    "                               bsz_objects=16,\n",
    "                               bsz_images=576,\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb8d8",
   "metadata": {},
   "source": [
    "## Run Model Inference\n",
    "\n",
    "- We perform the individual steps (detector, coarse, refiner, scoring) etc. separately to make the inference pipeline transparent. You can simply use pose_estimator.run_inference_pipeline to run them all at once.\n",
    "- You can set the options as to whether to use the ground-truth detections or the detections from Mask-RCNN.\n",
    "- Note: If you aren't using gt_detections and there are multiple object instances in the scene this won't work properly.\n",
    "- If you are getting CUDA out of memory errors decrease `bsz_objects` and `bsz_images` to smaller values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for inference\n",
    "use_gt_detections = True # Note, if you aren't using gt_detections then this should be false\n",
    "n_refiner_iterations = 5\n",
    "n_pose_hypotheses = 5\n",
    "return_debug_data = True\n",
    "detection_filter_kwargs = {'labels': [object_label]}\n",
    "run_depth_refiner = False\n",
    "\n",
    "\n",
    "bsz_images = 128\n",
    "bsz_objects = 2\n",
    "\n",
    "\n",
    "pose_estimator.bsz_objects = bsz_objects\n",
    "pose_estimator.bsz_images = bsz_images\n",
    "\n",
    "# set the random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    if use_gt_detections:\n",
    "        detections = gt_detections\n",
    "    else:\n",
    "        # Only keep the top detection in each image\n",
    "        detections = pose_estimator.forward_detection_model(observation_tensor, one_instance_per_class=True)\n",
    "        \n",
    "    # Filter and only run the estimator for that object\n",
    "    \n",
    "    detections = megapose.inference.utils.filter_detections(detections, **detection_filter_kwargs)\n",
    "    detections = megapose.inference.utils.add_instance_id_to_detections(detections)\n",
    "    detections = detections.cuda()\n",
    "    \n",
    "\n",
    "#     print(\"detections\\n\", detections)\n",
    "    \n",
    "    # We have split the inference into it's component steps for clarity. This is a copy of\n",
    "    # what is in the pose_estimator.run_pipeline method\n",
    "    # Run the coarse estimator using gt_detections\n",
    "    data_TCO_coarse, extra_data = pose_estimator.forward_coarse_model(observation=observation_tensor,\n",
    "                                       detections=detections, cuda_timer=True)\n",
    "    \n",
    "    print(f\"Forward Coarse: total={extra_data['time']:.2f}, \"\\\n",
    "          f\"model_time={extra_data['model_time']:.2f}, render_time={extra_data['render_time']:.2f}\")\n",
    "    \n",
    "    # Extract top-K coarse hypotheses\n",
    "    data_TCO_filtered = pose_estimator.filter_pose_estimates(data_TCO_coarse, \n",
    "                                                             top_K=n_pose_hypotheses, \n",
    "                                                             filter_field='coarse_logit')\n",
    "    \n",
    "    # Refine the top_K coarse hypotheses\n",
    "    preds, extra_data = pose_estimator.forward_refiner(observation_tensor, data_TCO_filtered, \n",
    "                                                   n_iterations=n_refiner_iterations, keep_all_outputs=True)\n",
    "    \n",
    "    print(f\"Refiner time: {extra_data['time']:.2f}\")\n",
    "    data_TCO_refined = preds[f'iteration={n_refiner_iterations}']\n",
    "    refiner_preds = preds\n",
    "    refiner_outputs = extra_data['outputs']\n",
    "    \n",
    "    # Score the refined poses using the coarse model.\n",
    "    data_TCO_scored, extra_data = pose_estimator.forward_scoring_model(observation_tensor, data_TCO_refined)\n",
    "\n",
    "    # Extract the highest scoring pose estimate for each instance_id\n",
    "    data_TCO_final = pose_estimator.filter_pose_estimates(data_TCO_scored, top_K=1, filter_field='pose_logit')\n",
    "    \n",
    "    \n",
    "    if run_depth_refiner:\n",
    "        print(\"\\n\\n\")\n",
    "        t = time.time()\n",
    "        data_TCO_depth_refiner, _ = pose_estimator.run_depth_refiner(observation_tensor, data_TCO_final,\n",
    "                                                                    )\n",
    "        depth_refiner_time = time.time() - t\n",
    "    else:\n",
    "        data_TCO_depth_refiner = None\n",
    "        \n",
    "    \n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Entire pose estimation pipeline took {elapsed:.2f} seconds\")\n",
    "\n",
    "print(\"Final Pose Estimate\\n\")\n",
    "print(data_TCO_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2061433",
   "metadata": {},
   "source": [
    "## Run the entire pipeline\n",
    "\n",
    "- The cell below shows how to run the entire pipeline in one function call, rather than each step individually.\n",
    "- It is disabled by default, set the flag to `True` to run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    use_gt_detections = False\n",
    "    if use_gt_detections:\n",
    "        detections_in = gt_detections.cuda()\n",
    "        run_detector=False\n",
    "    else:\n",
    "        detections_in = None\n",
    "        run_detector=True\n",
    "\n",
    "\n",
    "    detection_filter_kwargs = {'labels': [object_label], 'one_instance_per_class':True}\n",
    "\n",
    "    data_TCO_out, pred_data = pose_estimator.run_inference_pipeline(observation_tensor,\n",
    "                                         detections=detections_in,\n",
    "                                         run_detector=run_detector,\n",
    "                                         n_refiner_iterations=5,\n",
    "                                         n_pose_hypotheses=5,\n",
    "                                        detection_filter_kwargs=detection_filter_kwargs,\n",
    "                                        cuda_timer=True)\n",
    "    \n",
    "    print(f\"Inference pipeline: {pred_data['timing_str']}\")\n",
    "    print(f\"Coarse model: {pred_data['coarse']['data']['timing_str']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039fcdc",
   "metadata": {},
   "source": [
    "## Extract data from refiner iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c05de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_cosypose(data, data_TCO_final, refiner_outputs_in, object_label, \n",
    "                  plot_iter=[1, 2, 3, 4], scene_data=None):\n",
    "#     orig_renderer = object_predictor.pose_predictor.refiner_model.renderer\n",
    "#     object_predictor.pose_predictor.refiner_model.renderer = renderer\n",
    "    \n",
    "    rows = []\n",
    "    outputs = []\n",
    "    \n",
    "    df = data_TCO_final.infos\n",
    "    df_filter = df[df.label == object_label]\n",
    "    assert len(df_filter) == 1, f\"There was more than one object named {object_label} in refiner_preds\"\n",
    "    \n",
    "    refiner_batch_idx = df_filter.iloc[0]['refiner_batch_idx']\n",
    "    refiner_instance_idx = df_filter.iloc[0][\"refiner_instance_idx\"]\n",
    "    \n",
    "\n",
    "    df_gt = data['gt_detections'].infos\n",
    "    df_gt_filter = df_gt[df_gt.label == object_label]\n",
    "    \n",
    "    assert len(df_gt_filter) == 1, f\"There was more than one object named {object_label} in data['gt_detections']\"\n",
    "    obj_idx_gt = df_gt_filter.iloc[0].name\n",
    "    TWC = scene_data.camera_data.TWC.matrix\n",
    "    TCO_gt = data['gt_detections'].poses[obj_idx_gt].cpu().numpy().astype(np.float64)\n",
    "    TOC_gt = np.linalg.inv(TCO_gt)\n",
    "    \n",
    "    \n",
    "    if 'data_TCO_init' in all_preds:\n",
    "        data_TCO_init = all_preds['data_TCO_init']\n",
    "        df = data_TCO_init.infos\n",
    "        df = df[df.label == object_label]\n",
    "        idx_tmp = df.index[0]\n",
    "        TCO_coarse_init = cast_to_numpy(data_TCO_init.poses[idx_tmp], np.float64)\n",
    "    else:\n",
    "        TCO_coarse_init = None\n",
    "    \n",
    "    \n",
    "\n",
    "    for n in plot_iter:\n",
    "        refiner_outputs_iter = refiner_outputs_in[refiner_batch_idx][f'iteration={n}']\n",
    "        image_crop = refiner_outputs[refiner_batch_idx][f'iteration={n}']['images_crop']\\\n",
    "            [refiner_instance_idx][RGB_DIMS]\n",
    "        render_crop = refiner_outputs[refiner_batch_idx][f'iteration={n}']['renders']\\\n",
    "            [refiner_instance_idx][RGB_DIMS]\n",
    "\n",
    "        image_crop = (image_crop.permute(1, 2, 0) * 255).cpu().numpy().astype(np.uint8)\n",
    "        render_crop = (render_crop.permute(1, 2, 0) * 255).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        image_f = plotter.plot_image(image_crop)\n",
    "        render_f = plotter.plot_image(render_crop)\n",
    "        overlay_f = plotter.plot_overlay(image_crop, render_crop)\n",
    "        row = [image_f, render_f, overlay_f]\n",
    "        TCO_pred = refiner_outputs[refiner_batch_idx][f'iteration={n}']['TCO_input'][refiner_instance_idx].cpu().numpy()\n",
    "        TCO_output = refiner_outputs[refiner_batch_idx][f'iteration={n}']['TCO_input'][refiner_instance_idx].cpu().numpy()\n",
    "        \n",
    "        \n",
    "        # compute errors\n",
    "        TCO_pred = orthogonalize_rotation(TCO_pred)\n",
    "        TOgt_O = np.linalg.inv(TCO_gt) @ TCO_pred\n",
    "        TOgt_O = orthogonalize_rotation(TOgt_O)\n",
    "        trans_err = np.linalg.norm(TOgt_O[:3,3])\n",
    "        \n",
    "        \n",
    "        # Compute coarse score\n",
    "        rgb = data['rgb']\n",
    "        depth = data['depth']\n",
    "\n",
    "        # [B,C,H,W], C=3 or 4 depending on if depth was empty or not\n",
    "        # Compute score from coarse model\n",
    "        images = cast_images_to_tensor(rgb, depth)\n",
    "        K = data['cameras'].K.cuda().float()\n",
    "        label = [object_label]\n",
    "        TCO_pred_tensor = torch.tensor(TCO_pred).cuda().unsqueeze(0)\n",
    "        out_ = coarse_model.forward_coarse(images, K, label, TCO_input=TCO_pred_tensor, \n",
    "                                           return_debug_data=True)\n",
    "        \n",
    "        coarse_out = out_\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            _, rot_err_angle_radians = transforms3d.axangles.mat2axangle(TOgt_O[:3,:3])\n",
    "            rot_err_deg = np.rad2deg(np.abs(rot_err_angle_radians))\n",
    "        except ValueError:\n",
    "            print(\"got error while computing angle distance\")\n",
    "            rot_err_deg = -1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        infos = dict(figures=row, \n",
    "                     TCO_output=TCO_output,\n",
    "                     TCO_input=TCO_pred,\n",
    "                     TCO_gt=TCO_gt,\n",
    "                     TOC_gt=TOC_gt,\n",
    "                     TOgt_O=TOgt_O,\n",
    "                     label=object_label,\n",
    "                     refiner_batch_idx=refiner_batch_idx,\n",
    "                     refiner_instance_idx=refiner_instance_idx,\n",
    "                     iteration=n,\n",
    "                     refiner_outputs=refiner_outputs_iter,\n",
    "                     scene_data=scene_data,\n",
    "                     input_rgb_dims=copy.copy(refiner_model.input_rgb_dims),\n",
    "                     input_depth_dims=copy.copy(refiner_model.input_depth_dims),\n",
    "                     render_rgb_dims=copy.copy(refiner_model.render_rgb_dims),\n",
    "                     render_depth_dims=copy.copy(refiner_model.render_depth_dims),\n",
    "                     TCO_coarse_init=TCO_coarse_init,\n",
    "                     trans_err=trans_err,\n",
    "                     rot_err=rot_err_deg,\n",
    "                     coarse_out=coarse_out,\n",
    "#                      TCO_coarse_init=TCO_coarse_init,\n",
    "#                      object_predictor_data=object_predictor_data,\n",
    "                    )\n",
    "        outputs.append(infos)\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_iter = [1, 2, 3,4,5,6]\n",
    "plot_iter = list(range(1, n_refiner_iterations+1))\n",
    "# plot_iter = [1,2,3,4,5,6,7,8]\n",
    "all_preds = preds\n",
    "all_infos = plot_cosypose(data, data_TCO_final, refiner_outputs,\n",
    "                          object_label, plot_iter, scene_data=scene_data)\n",
    "\n",
    "for info in all_infos:\n",
    "    info['result_name'] = result_name\n",
    "all_infos = pd.DataFrame(all_infos)\n",
    "\n",
    "# save_path = NB_DATA_DIR / f'{result_name}_ds_name={ds_name}_scene_id={scene_id}_im={view_id}_object_label={object_label}.pkl'\n",
    "# save_path.write_bytes(pkl.dumps(all_infos.drop(columns=('figures'))))\n",
    "# print(\"wrote\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69819c",
   "metadata": {},
   "source": [
    "## Make contour overlay figure\n",
    "\n",
    "Overlay ground-truth and estimated pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19771898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAVE_DIR = NB_DATA_DIR/'figures'\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "ambient_light_data = Panda3dLightData('ambient')\n",
    "light_datas = [[ambient_light_data]]\n",
    "\n",
    "\n",
    "# Need to render an image at the ground-truth pose\n",
    "d = dict()\n",
    "for n in [n_refiner_iterations]:\n",
    "    \n",
    "    # Initial coarse estimate\n",
    "    x = refiner_outputs[0][f'iteration={n}']\n",
    "    render_img_tensor = x['renders'][0,0:3]\n",
    "    render_img = tensor_image_to_uint8(render_img_tensor)\n",
    "    render_img_PIL = Image.fromarray(render_img)\n",
    "    render_img_PIL = adjust_brightness(render_img_PIL, factor=BRIGHTNESS_FACTOR)\n",
    "    render_img_PIL.save(f'{SAVE_DIR}/refiner_iter={n}_render.png')\n",
    "\n",
    "\n",
    "    img_tensor = x['images_crop'][0, 0:3]\n",
    "    img = tensor_image_to_uint8(img_tensor)\n",
    "    img_PIL = Image.fromarray(img)\n",
    "\n",
    "    img_PIL.save(SAVE_DIR/f\"refiner_iter={n}_img_crop.png\")\n",
    "\n",
    "    blend = Plotter.make_overlay(img, np.array(render_img_PIL))\n",
    "    \n",
    "    contour_out = make_contour_overlay(img, np.array(render_img_PIL), dilate_iterations=1, color=[0,255,0])\n",
    "    contour = contour_out['img']\n",
    "    \n",
    "    contour_both = make_contour_overlay(img, np.array(render_img_PIL), color=[255,0,0],\n",
    "                                       dilate_iterations=0)['img']\n",
    "\n",
    "    \n",
    "    ### Render image at the ground-truth pose #######\n",
    "    # [1,3,3]\n",
    "    pred_idx = 0\n",
    "    K = x['K_crop'][pred_idx].unsqueeze(0)\n",
    "    \n",
    "    df = all_infos\n",
    "    df = df[df.iteration==n]\n",
    "\n",
    "    # [1,4,4]\n",
    "    TCO_gt = torch.tensor(df.iloc[0].TCO_gt).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "#     if n > 1:\n",
    "#         print(\"TCO:\\n\", x['TCO_input'])\n",
    "#         print(\"TCO_gt:\\n\", TCO_gt)\n",
    "        \n",
    "#     TCO_gt = x['TCO_output'][pred_idx].unsqueeze(0)\n",
    "    obj_infos = [{'name': x['labels'][pred_idx]}]\n",
    "#     print(\"obj_infos\", obj_infos)\n",
    "\n",
    "    \n",
    "\n",
    "    render_out = renderer.render(labels=[object_label],\n",
    "                                 TCO=TCO_gt,\n",
    "                                 K=K,\n",
    "                                 resolution=img.shape[:2],\n",
    "                                 light_datas=light_datas)\n",
    "    \n",
    "    \n",
    "    render_img_gt_tensor = render_out.rgbs[0]\n",
    "    render_img_gt = tensor_image_to_uint8(render_img_gt_tensor)\n",
    "\n",
    "    render_img_gt_PIL = Image.fromarray(render_img_gt)\n",
    "    render_img_gt_PIL = adjust_brightness(render_img_gt_PIL, factor=BRIGHTNESS_FACTOR)\n",
    "    render_img_gt_PIL.save(f'{SAVE_DIR}/refiner_iter={n}_render_gt_pose.png')\n",
    "    \n",
    "    \n",
    "    contour_both = make_contour_overlay(contour_both, np.array(render_img_gt_PIL), color=[0,255,0],\n",
    "                                       dilate_iterations=0)['img']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    contour_both_PIL = Image.fromarray(contour_both)\n",
    "    contour_both_PIL.save(f'{SAVE_DIR}/refiner_iter={n}_contour_both.png')\n",
    "    \n",
    "    \n",
    "    if data_TCO_depth_refiner is not None:\n",
    "        df = data_TCO_depth_refiner.infos\n",
    "        df = df[df.label == object_label]\n",
    "        assert len(df) == 1, f\"Found more than one prediction with label {object_label}\"\n",
    "        TCO = data_TCO_depth_refiner.poses[df.index.tolist()]\n",
    "        render_out = renderer.render(labels=[object_label],\n",
    "                                 TCO=TCO,\n",
    "                                 K=K,\n",
    "                                 resolution=img.shape[:2],\n",
    "                                 light_datas=light_datas)\n",
    "        render_img_depth_refiner_tensor = render_out.rgbs[0]\n",
    "        render_img_depth_refiner = tensor_image_to_uint8(render_img_gt_tensor)\n",
    "        contour_depth_refiner = make_contour_overlay(img, render_img_depth_refiner, dilate_iterations=1, color=[0,255,0])\n",
    "        \n",
    "    else:\n",
    "        contour_depth_refiner = None\n",
    "        \n",
    "\n",
    "    \n",
    "    d[n] = {'render': np.array(render_img_PIL),\n",
    "           'img': img,\n",
    "           'blend': blend,\n",
    "           'contour': contour,\n",
    "           'contour_out': contour_out,\n",
    "           'render_gt': np.array(render_img_gt_PIL),\n",
    "           'contour_both': np.array(contour_both_PIL),\n",
    "            'contour_depth_refiner': contour_depth_refiner,\n",
    "           }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(d[n_refiner_iterations]['img'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(d[n_refiner_iterations]['render'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(d[n_refiner_iterations]['contour_out']['img'])\n",
    "plt.title(\"Megapose Refiner\")\n",
    "plt.show()\n",
    "\n",
    "if d[n_refiner_iterations]['contour_depth_refiner'] is not None:\n",
    "    plt.figure()\n",
    "    plt.imshow(d[n_refiner_iterations]['contour_depth_refiner']['img'])\n",
    "    plt.title(\"Megapose + Depth refiner\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d367460",
   "metadata": {},
   "source": [
    "## Visualize refiner iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058eb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"scene_id: {scene_id}, view_id: {im_idx}, object_label: {object_label}\")\n",
    "grid = []\n",
    "# plot_iter = [1,2,3,4,5,6,7,8]\n",
    "plot_object_label = [object_label]\n",
    "df = all_infos.copy()\n",
    "df = df.loc[(df['iteration'].isin(plot_iter)) & (df['label'] == object_label)]\n",
    "for _, row in df.iterrows():\n",
    "    figures = row['figures']\n",
    "    result_name = row['result_name']\n",
    "    logit = float(row['coarse_out']['logits'][0])\n",
    "    score = float(row['coarse_out']['scores'][0])\n",
    "    k = row['iteration']\n",
    "    figures[1].title.text = f'{result_name} / iter={k} / logit={logit:.1f}, score={score:.1f} '\n",
    "    figures[2].title.text_font_size = '12pt'\n",
    "    grid.append(figures)\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b9200",
   "metadata": {},
   "source": [
    "## 3D visualization using meshcat.\n",
    "\n",
    "Make sure you have a `meshcat-server` process running on the host machine. Otherwise this code will hang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megapose.visualization import meshcat_utils\n",
    "from megapose.visualization.meshcat_visualizer import MeshcatSceneViewer\n",
    "import meshcat.geometry as g\n",
    "vis = meshcat_utils.create_visualizer()\n",
    "\n",
    "df = all_infos\n",
    "show_iter = [n_refiner_iterations]\n",
    "df = df.loc[(df['iteration'].isin(show_iter))]\n",
    "\n",
    "viewer = MeshcatSceneViewer(obj_ds_name, use_textures=True,)\n",
    "# viewer = MeshcatSceneViewer('ycbv', use_textures=False) # debugging\n",
    "vis = viewer.visualizer\n",
    "\n",
    "def extract_pointcloud_from_scene_data(scene_data):\n",
    "    depth = scene_data.depth\n",
    "    K = scene_data.camera_data.K\n",
    "    pc = meshcat_utils.get_pointcloud(depth, K)    \n",
    "    \n",
    "    return pc\n",
    "\n",
    "\n",
    "def extract_pointclouds(iter_info):\n",
    "    refiner_outputs = iter_info['refiner_outputs']\n",
    "    unique_id = iter_info['refiner_instance_idx']\n",
    "    image_crop_raw = refiner_outputs['images_crop_raw'][unique_id]\n",
    "    render_raw = refiner_outputs['renders_raw'][unique_id]\n",
    "    K_crop = refiner_outputs['K_crop'][unique_id].cpu().numpy()\n",
    "    KV_crop = refiner_outputs['KV_crop'][unique_id][0].cpu().numpy()\n",
    "    TCO_input = refiner_outputs['TCO_input'][unique_id].cpu().numpy()\n",
    "\n",
    "    input_depth_dims = iter_info['input_depth_dims']\n",
    "    render_depth_dims = iter_info['render_depth_dims']\n",
    "    if len(input_depth_dims) > 0:\n",
    "        input_depth = image_crop_raw[input_depth_dims].permute(1,2,0).cpu().squeeze().numpy()\n",
    "        gt_pc = meshcat_utils.get_pointcloud(input_depth, K_crop)    \n",
    "    else:\n",
    "        gt_pc = None\n",
    "        \n",
    "        \n",
    "    if len(render_depth_dims) > 0:\n",
    "        render_depth = render_raw[render_depth_dims].permute(1,2,0).cpu().squeeze().numpy()\n",
    "        render_pc = meshcat_utils.get_pointcloud(render_depth, KV_crop)    \n",
    "    else:\n",
    "        render_pc = None\n",
    "        \n",
    "        \n",
    "    return {'gt': gt_pc,\n",
    "           'render': render_pc,\n",
    "            'TCO_input': TCO_input,\n",
    "           }\n",
    "        \n",
    "def plot_results(df):\n",
    "    df = df.to_dict('records')\n",
    "    \n",
    "    obj_infos = []\n",
    "    TCO_gt = df[0]['TCO_gt']\n",
    "    TOC_gt = np.linalg.inv(TCO_gt)\n",
    "    obj_label = df[0]['label']\n",
    "    # Visualize camera\n",
    "        \n",
    "    for row in df:\n",
    "        meshcat_prefix = f\"{row['result_name']}\"\n",
    "        TOgt_O = np.linalg.inv(TCO_gt) @ row['TCO_input']\n",
    "        k = f\"{row['result_name']}/iteration={row['iteration']}/mesh\"\n",
    "        obj_infos.append({'name': obj_label, 'TWO': TOgt_O, 'node_name': k})\n",
    "        \n",
    "        \n",
    "    if data_TCO_depth_refiner is not None:\n",
    "        df_ = data_TCO_depth_refiner.infos\n",
    "        idx = df_[df_.label == object_label].iloc[0].name\n",
    "        TCO_depth_refiner = data_TCO_depth_refiner.poses[idx].cpu().numpy()\n",
    "        TOgt_O = np.linalg.inv(TCO_gt) @ TCO_depth_refiner\n",
    "        k = f\"{row['result_name']}/depth_refiner/mesh\"\n",
    "        obj_infos.append({'name': obj_label, 'TWO': TOgt_O, 'node_name': k})\n",
    "        \n",
    "        \n",
    "#     if 'TCO_coarse_init' in df[0]:\n",
    "#         TCO_coarse_init = cast_to_numpy(df[0]['TCO_coarse_init'], np.float64)\n",
    "#         TOgt_O = TOC_gt @ TCO_coarse_init\n",
    "#         obj_infos.append({'name': obj_label, 'TWO': TOgt_O, 'node_name': f'{meshcat_prefix}/TCO_coarse_init/mesh'})\n",
    "        \n",
    "        \n",
    "    obj_infos.append({'name': obj_label, 'TWO': np.eye(4), 'node_name': 'ground_truth'})\n",
    "    viewer.visualize_scene(obj_infos)\n",
    "    \n",
    "    # Extra visualization must be after the 'visualize_scene' call\n",
    "    meshcat_utils.make_frame(vis, \"camera\", transform=TOC_gt, ignore_invalid_transform=True)\n",
    "    \n",
    "    # Line connecting camera and origin\n",
    "    vertices = np.zeros([3,2])\n",
    "    vertices[:, 1] = TOC_gt[:3,3]\n",
    "    vis['line'].set_object(g.Line(g.PointsGeometry(vertices)))\n",
    "    \n",
    "    \n",
    "    # visualize ground-truth pointcloud\n",
    "    pc_gt = extract_pointcloud_from_scene_data(df[0]['scene_data'])\n",
    "    meshcat_utils.visualize_pointcloud(vis, 'ground_truth_pointcloud', pc_gt, transform=TOC_gt,\n",
    "                                              color=[0,255,0])\n",
    "    \n",
    "    \n",
    "    # visualize depth images, but only for final index\n",
    "    for row in df:\n",
    "        pc_data = extract_pointclouds(row)\n",
    "        f\"{row['result_name']}/iteration={row['iteration']}\"\n",
    "#         if pc_data['gt'] is not None:\n",
    "#             print(\"Visualizing ground-truth pointcloud\")\n",
    "#             meshcat_utils.visualize_pointcloud(vis, 'ground_truth_pointcloud', pc_data['gt'], transform=TOC_gt,\n",
    "#                                               color=[0,255,0])\n",
    "            \n",
    "        if pc_data['render'] is not None:\n",
    "            TOgt_O = np.linalg.inv(TCO_gt) @ row['TCO_input']\n",
    "            TCO_input = pc_data['TCO_input']\n",
    "            TOC_input = np.linalg.inv(TCO_input)\n",
    "            TOgt_input = TOgt_O @ TOC_input # transform rendered to observed\n",
    "            \n",
    "            k = f\"{row['result_name']}/iteration={row['iteration']}/pointcloud\"\n",
    "            meshcat_utils.visualize_pointcloud(vis, k, pc_data['render'],\n",
    "                                               transform=TOC_gt, color=[255,255,0])\n",
    "        \n",
    "    return\n",
    "\n",
    "plot_results(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79c7aa",
   "metadata": {},
   "source": [
    "## Print accuracy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_infos\n",
    "df_first_iter = df[df.iteration==1]\n",
    "trans_err_init = float(df_first_iter.trans_err)\n",
    "rot_err_init = float(df_first_iter.rot_err)\n",
    "last_iter = df.iteration.max()\n",
    "# print(\"last_iter\", last_iter)\n",
    "df_final_iter = df[df.iteration==df.iteration.max()]\n",
    "\n",
    "trans_err = float(df_final_iter.trans_err)\n",
    "rot_err = float(df_final_iter.rot_err)\n",
    "# print(df_final_iter.tran)\n",
    "# df_final_iter = df_final_iter.iloc[0]\n",
    "\n",
    "SO3_grid_size = pose_estimator._SO3_grid.shape[0]\n",
    "# print(x_failure)\n",
    "print(f\"result_name: {result_name}, SO3-grid-size={SO3_grid_size}\")\n",
    "print(f\"iteration={last_iter}\")\n",
    "print(f\"per_iter_depth_multiplier:\", refiner_model.per_iter_depth_multiplier)\n",
    "print(\"depth_multiplier:\", refiner_model.depth_multiplier)\n",
    "print(f\"\\ninitial translation error (cm): {trans_err_init*100:.2f}\")\n",
    "print(f\"initial rot_err (no sym) (deg): {rot_err_init:.1f}\")\n",
    "print(f\"\\ntranslation error (cm): {trans_err*100:.2f}\\nrot_err (no sym) (deg): {rot_err:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f8e12",
   "metadata": {},
   "source": [
    "##### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab857fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e22c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
